{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PAVICS Web Processing Services \n",
    "PAVICS allows access to a number of different WPS services via Birdhouse\n",
    "* Each 'bird' groups a set of processing tools \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from owslib.wps import WebProcessingService\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### One suite of WPS tools for netcdf files resides in 'Hummingbird'\n",
    "For metadata use GetCapabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hummingbird 0.5_dev'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hummingbird WPS url\n",
    "wps_url = 'https://pavics.ouranos.ca/twitcher/ows/proxy/hummingbird/wps'\n",
    "# connection\n",
    "wps = WebProcessingService(url=wps_url)\n",
    "# print wps title\n",
    "wps.identification.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Print out info on available processes (from Hummingbird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncdump \t : Run ncdump to retrieve NetCDF header metadata. \n\nspotchecker \t : Checks a single uploaded or remote dataset against a variety of compliance standards. The dataset is either in the NetCDF format or a remote OpenDAP resource. Available compliance standards are the Climate and Forecast conventions (CF) and project specific rules for CMIP6 and CORDEX. \n\ncchecker \t : Runs the IOOS Compliance Checker tool to check datasets against compliance standards. Each compliance standard is executed by a Check Suite, which functions similar to a Python standard Unit Test. A Check Suite runs one or more checks against a dataset, returning a list of Results which are then aggregated into a summary. Development and maintenance for the compliance checker is done by the Integrated Ocean Observing System (IOOS). \n\ncfchecker \t : The NetCDF Climate Forcast Conventions compliance checker by CEDA. This process allows you to run the compliance checker to check that the contents of a NetCDF file comply with the Climate and Forecasts (CF) Metadata Convention. The CF-checker was developed at the Hadley Centre for Climate Prediction and Research, UK Met Office by Rosalyn Hatcher. This work was supported by PRISM (PRogramme for Integrated Earth System Modelling). Development and maintenance for the CF-checker has now been taken over by the NCAS Computational Modelling Services (NCAS-CMS). If you have suggestions for improvement then please contact Rosalyn Hatcher at NCAS-CMS (r.s.hatcher@reading.ac.uk). \n\ncmor_checker \t : Calls the CMIP6 cmor checker to verify CMIP6 compliance.CMIP6 CMOR checker will verify that all attributes in the input file are present and conform to CMIP6 for publication into ESGF. \n\nqa_cfchecker \t : The NetCDF Climate Forcast Conventions compliance checker by DKRZ. This process allows you to run the compliance checker to check that the contents of a NetCDF file comply with the Climate and Forecasts (CF) Metadata Convention. The CF Conformance checker applies to conventions 1.4 -1.7draft. Development and maintenance for the CF-checker is done by the German Climate Computing Centre (DKRZ). If you have suggestions for improvement then please contact Heinz-Dieter Hollweg at DKRZ (hollweg@dkrz.de). \n\nqa_checker \t : The Quality Assurance checker QA-DKRZ checks conformance of meta-data of climate simulations given in NetCDF format with conventions and rules of climate model projects. At present, checking of CF Conventions, CMIP5, and CORDEX is supported. Development and maintenance for the QA checker is done by the German Climate Computing Centre (DKRZ). If you have suggestions for improvement then please contact Heinz-Dieter Hollweg at DKRZ (hollweg@dkrz.de). \n\ncdo_sinfo \t : Runs CDO to retrieve NetCDF metadata information. Calls the sinfo operator of CDO (Climate Data Operator) on a NetCDF file and returns a document with metadata information. \n\ncdo_operation \t : Calls CDO operations like monmax on a NetCDF file. \n\ncdo_copy \t : Calls CDO to copy or concatenate datasets. All input datasets need to have the same structure with the same variables on different timesteps. \n\ncdo_bbox \t : Runs CDO to clip a bounding-box from a NetCDF file. Calls the CDO (Climate Data Operators) sellonlatbox operator with a bounding-box and returns the resulting NetCDF file. \n\ncdo_indices \t : Calculates climate indices like summer days using CDO. Calls the Climate Data Operators (CDO) tool with a single dataset (NetCDF, OpenDAP) provided and uses the chosen operator to calculate climate indices written to a NetCDF file. \n\nensembles \t : Calling cdo to calculate ensembles operations. \n\ncdo_inter_mpi \t : CDO Remapping of NetCDF File(s) with multiprocessing \n\n"
     ]
    }
   ],
   "source": [
    "for process in wps.processes:\n",
    "    print ('%s \\t : %s \\n' %(process.identifier, process.abstract))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAVICS/Hummingbird has lots of WPS services\n",
    "### Let's keep it simple with 'ncdump'  \n",
    "* Print info on WPS inputs needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\ndataset_opendap\n"
     ]
    }
   ],
   "source": [
    "# ncdump\n",
    "proc_name = 'ncdump'\n",
    "process = wps.describeprocess(proc_name) # get process info\n",
    "for inputs in process.dataInputs:\n",
    "    print(inputs.identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The only input we need is a dataset (url) or it's OpenDAP link\n",
    "* A simple way to find a test dataset is to access : https://pavics.ouranos.ca/thredds\n",
    "\n",
    "* Note - PAVICS also has a catalogue WPS but we will see that in other examples later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/nrcan/nrcan_canada_daily/tasmin/nrcan_canada_daily_tasmin_2013.nc\n"
     ]
    }
   ],
   "source": [
    "# Exampe netcdf url to NRCAN daily - tasmin 2013\n",
    "nc_url = 'https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/nrcan/nrcan_canada_daily/tasmin/nrcan_canada_daily_tasmin_2013.nc'\n",
    "print(nc_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create WPS input - Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dataset_opendap', 'https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/birdhouse/nrcan/nrcan_canada_daily/tasmin/nrcan_canada_daily_tasmin_2013.nc')]\n"
     ]
    }
   ],
   "source": [
    "myinputs = []\n",
    "myinputs.append(('dataset_opendap',nc_url)) # inputs : use opendap link of a single netcdf file from catalogue search to erun ncdump\n",
    "print(myinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the WPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncdump\n"
     ]
    }
   ],
   "source": [
    "print(proc_name)\n",
    "execute = wps.execute(proc_name, myinputs)\n",
    "execute\n",
    "\n",
    "# Note- execute.processOutputs[-1].reference should provide url link directly but does not here?\n",
    "# execute.processOutputs[-1].reference   \n",
    "\n",
    "# Work around using statuslocation xml as ElementTree (ET) package \n",
    "o = requests.get(execute.statusLocation)\n",
    "tree = ET.fromstring(o.content)\n",
    "for child in tree:\n",
    "    if 'ProcessOutputs' in child.tag:\n",
    "        for c in child:\n",
    "            if 'Output' in c.tag:\n",
    "                for cc in c: \n",
    "                    if 'Reference' in cc.tag:\n",
    "                        r = requests.get(cc.get('{http://www.w3.org/1999/xlink}href'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get result and print to screen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\nNCdump results:\n\nnetcdf nrcan_canada_daily_tasmin_2013.nc {\ndimensions:\n\ttime = UNLIMITED ; // (365 currently)\n\tlat = 510 ;\n\tlon = 1068 ;\n\tts = 3 ;\nvariables:\n\tfloat lon(lon) ;\n\t\tlon:units = \"degrees_east\" ;\n\t\tlon:long_name = \"longitude\" ;\n\t\tlon:standard_name = \"longitude\" ;\n\t\tlon:axis = \"X\" ;\n\t\tlon:_ChunkSizes = 1068 ;\n\tfloat lat(lat) ;\n\t\tlat:axis = \"Y\" ;\n\t\tlat:units = \"degrees_north\" ;\n\t\tlat:long_name = \"latitude\" ;\n\t\tlat:standard_name = \"latitude\" ;\n\t\tlat:_ChunkSizes = 510 ;\n\tshort ts(ts) ;\n\t\tts:_FillValue = -32767s ;\n\t\tts:_ChunkSizes = 3 ;\n\tshort time(time) ;\n\t\ttime:axis = \"T\" ;\n\t\ttime:units = \"days since 1950-01-01 00:00:00\" ;\n\t\ttime:long_name = \"time\" ;\n\t\ttime:standard_name = \"time\" ;\n\t\ttime:calendar = \"gregorian\" ;\n\t\ttime:_ChunkSizes = 1 ;\n\tshort time_vectors(time, ts) ;\n\t\ttime_vectors:_ChunkSizes = 1, 3 ;\n\tfloat tasmin(time, lat, lon) ;\n\t\ttasmin:long_name = \"air_temperature\" ;\n\t\ttasmin:standard_name = \"air_temperature\" ;\n\t\ttasmin:units = \"K\" ;\n\t\ttasmin:_FillValue = 9.96921e+36f ;\n\t\ttasmin:_ChunkSizes = 31, 102, 267 ;\n\n// global attributes:\n\t\t:Conventions = \"CF-1.5\" ;\n\t\t:title = \"NRCAN 10km Gridded Climate Dataset\" ;\n\t\t:history = \"2016-01-05T16:30:06: Convert from original format to NetCDF\" ;\n\t\t:institution = \"NRCAN\" ;\n\t\t:source = \"ANUSPLIN\" ;\n\t\t:redistribution = \"Redistribution policy unknown. For internal use only.\" ;\n\t\t:DODS_EXTRA.Unlimited_Dimension = \"time\" ;\n}\n\n\n"
     ]
    }
   ],
   "source": [
    "execute.statusLocation\n",
    "print('\\n\\nNCdump results:\\n\\n' + r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
